"""
æ—¥å¿—å¯¼å‡ºæœåŠ¡
æä¾›æ—¥å¿—æ–‡ä»¶çš„æŸ¥è¯¢ã€è¿‡æ»¤å’Œå¯¼å‡ºåŠŸèƒ½
"""

import logging
import os
import zipfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict, Any
import re
import json

logger = logging.getLogger("webapi")


class LogExportService:
    """æ—¥å¿—å¯¼å‡ºæœåŠ¡"""

    def __init__(self, log_dir: str = "./logs"):
        """
        åˆå§‹åŒ–æ—¥å¿—å¯¼å‡ºæœåŠ¡
        
        Args:
            log_dir: æ—¥å¿—æ–‡ä»¶ç›®å½•
        """
        self.log_dir = Path(log_dir)
        if not self.log_dir.exists():
            logger.warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_dir}")
            self.log_dir.mkdir(parents=True, exist_ok=True)

    def list_log_files(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        
        Returns:
            æ—¥å¿—æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«æ–‡ä»¶åã€å¤§å°ã€ä¿®æ”¹æ—¶é—´ç­‰ä¿¡æ¯
        """
        log_files = []
        
        try:
            for file_path in self.log_dir.glob("*.log*"):
                if file_path.is_file():
                    stat = file_path.stat()
                    log_files.append({
                        "name": file_path.name,
                        "path": str(file_path),
                        "size": stat.st_size,
                        "size_mb": round(stat.st_size / (1024 * 1024), 2),
                        "modified_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "type": self._get_log_type(file_path.name)
                    })
            
            # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åº
            log_files.sort(key=lambda x: x["modified_at"], reverse=True)
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
            return log_files
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _get_log_type(self, filename: str) -> str:
        """
        æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ—¥å¿—ç±»å‹
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            æ—¥å¿—ç±»å‹
        """
        if "error" in filename.lower():
            return "error"
        elif "webapi" in filename.lower():
            return "webapi"
        elif "worker" in filename.lower():
            return "worker"
        elif "access" in filename.lower():
            return "access"
        else:
            return "other"

    def read_log_file(
        self,
        filename: str,
        lines: int = 1000,
        level: Optional[str] = None,
        keyword: Optional[str] = None,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è¯»å–æ—¥å¿—æ–‡ä»¶å†…å®¹ï¼ˆæ”¯æŒè¿‡æ»¤ï¼‰
        
        Args:
            filename: æ—¥å¿—æ–‡ä»¶å
            lines: è¯»å–çš„è¡Œæ•°ï¼ˆä»æœ«å°¾å¼€å§‹ï¼‰
            level: æ—¥å¿—çº§åˆ«è¿‡æ»¤ï¼ˆERROR, WARNING, INFO, DEBUGï¼‰
            keyword: å…³é”®è¯è¿‡æ»¤
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            
        Returns:
            æ—¥å¿—å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯
        """
        file_path = self.log_dir / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                all_lines = f.readlines()
            
            # ä»æœ«å°¾å¼€å§‹è¯»å–æŒ‡å®šè¡Œæ•°
            recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
            
            # åº”ç”¨è¿‡æ»¤å™¨
            filtered_lines = []
            stats = {
                "total_lines": len(all_lines),
                "filtered_lines": 0,
                "error_count": 0,
                "warning_count": 0,
                "info_count": 0,
                "debug_count": 0
            }
            
            for line in recent_lines:
                # ç»Ÿè®¡æ—¥å¿—çº§åˆ«
                if "ERROR" in line:
                    stats["error_count"] += 1
                elif "WARNING" in line:
                    stats["warning_count"] += 1
                elif "INFO" in line:
                    stats["info_count"] += 1
                elif "DEBUG" in line:
                    stats["debug_count"] += 1
                
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if level and level.upper() not in line:
                    continue
                
                if keyword and keyword.lower() not in line.lower():
                    continue
                
                # æ—¶é—´è¿‡æ»¤ï¼ˆç®€å•å®ç°ï¼Œå‡è®¾æ—¥å¿—æ ¼å¼ä¸º YYYY-MM-DD HH:MM:SSï¼‰
                if start_time or end_time:
                    time_match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', line)
                    if time_match:
                        log_time = time_match.group()
                        if start_time and log_time < start_time:
                            continue
                        if end_time and log_time > end_time:
                            continue
                
                filtered_lines.append(line.rstrip())
            
            stats["filtered_lines"] = len(filtered_lines)
            
            return {
                "filename": filename,
                "lines": filtered_lines,
                "stats": stats
            }
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def export_logs(
        self,
        filenames: Optional[List[str]] = None,
        level: Optional[str] = None,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        format: str = "zip"
    ) -> str:
        """
        å¯¼å‡ºæ—¥å¿—æ–‡ä»¶
        
        Args:
            filenames: è¦å¯¼å‡ºçš„æ—¥å¿—æ–‡ä»¶ååˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå¯¼å‡ºæ‰€æœ‰ï¼‰
            level: æ—¥å¿—çº§åˆ«è¿‡æ»¤
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            format: å¯¼å‡ºæ ¼å¼ï¼ˆzip, txtï¼‰
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶çš„è·¯å¾„
        """
        try:
            # ç¡®å®šè¦å¯¼å‡ºçš„æ–‡ä»¶
            if filenames:
                files_to_export = [self.log_dir / f for f in filenames if (self.log_dir / f).exists()]
            else:
                files_to_export = list(self.log_dir.glob("*.log*"))
            
            if not files_to_export:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°è¦å¯¼å‡ºçš„æ—¥å¿—æ–‡ä»¶")
            
            # åˆ›å»ºå¯¼å‡ºç›®å½•
            export_dir = Path("./exports/logs")
            export_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆå¯¼å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if format == "zip":
                export_path = export_dir / f"logs_export_{timestamp}.zip"
                
                # åˆ›å»ºZIPæ–‡ä»¶
                with zipfile.ZipFile(export_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file_path in files_to_export:
                        # å¦‚æœæœ‰è¿‡æ»¤æ¡ä»¶ï¼Œå…ˆè¿‡æ»¤å†æ·»åŠ 
                        if level or start_time or end_time:
                            filtered_data = self.read_log_file(
                                file_path.name,
                                lines=999999,  # è¯»å–æ‰€æœ‰è¡Œ
                                level=level,
                                start_time=start_time,
                                end_time=end_time
                            )
                            # å°†è¿‡æ»¤åçš„å†…å®¹å†™å…¥ä¸´æ—¶æ–‡ä»¶
                            temp_file = export_dir / f"temp_{file_path.name}"
                            with open(temp_file, 'w', encoding='utf-8') as f:
                                f.write('\n'.join(filtered_data['lines']))
                            zipf.write(temp_file, file_path.name)
                            temp_file.unlink()  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        else:
                            zipf.write(file_path, file_path.name)
                
                logger.info(f"âœ… æ—¥å¿—å¯¼å‡ºæˆåŠŸ: {export_path}")
                return str(export_path)
            
            elif format == "txt":
                export_path = export_dir / f"logs_export_{timestamp}.txt"
                
                # åˆå¹¶æ‰€æœ‰æ—¥å¿—åˆ°ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶
                with open(export_path, 'w', encoding='utf-8') as outf:
                    for file_path in files_to_export:
                        outf.write(f"\n{'='*80}\n")
                        outf.write(f"æ–‡ä»¶: {file_path.name}\n")
                        outf.write(f"{'='*80}\n\n")
                        
                        if level or start_time or end_time:
                            filtered_data = self.read_log_file(
                                file_path.name,
                                lines=999999,
                                level=level,
                                start_time=start_time,
                                end_time=end_time
                            )
                            outf.write('\n'.join(filtered_data['lines']))
                        else:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as inf:
                                outf.write(inf.read())
                        
                        outf.write('\n\n')
                
                logger.info(f"âœ… æ—¥å¿—å¯¼å‡ºæˆåŠŸ: {export_path}")
                return str(export_path)
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
                
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")
            raise

    def get_log_statistics(self, days: int = 7) -> Dict[str, Any]:
        """
        è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            days: ç»Ÿè®¡æœ€è¿‘å‡ å¤©çš„æ—¥å¿—
            
        Returns:
            æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            cutoff_time = datetime.now() - timedelta(days=days)
            
            stats = {
                "total_files": 0,
                "total_size_mb": 0,
                "error_files": 0,
                "recent_errors": [],
                "log_types": {}
            }
            
            for file_path in self.log_dir.glob("*.log*"):
                if not file_path.is_file():
                    continue
                
                stat = file_path.stat()
                modified_time = datetime.fromtimestamp(stat.st_mtime)
                
                if modified_time < cutoff_time:
                    continue
                
                stats["total_files"] += 1
                stats["total_size_mb"] += stat.st_size / (1024 * 1024)
                
                log_type = self._get_log_type(file_path.name)
                stats["log_types"][log_type] = stats["log_types"].get(log_type, 0) + 1
                
                # ç»Ÿè®¡é”™è¯¯æ—¥å¿—
                if log_type == "error":
                    stats["error_files"] += 1
                    # è¯»å–æœ€è¿‘çš„é”™è¯¯
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            lines = f.readlines()
                            error_lines = [line for line in lines[-100:] if "ERROR" in line]
                            stats["recent_errors"].extend(error_lines[-10:])
                    except Exception:
                        pass
            
            stats["total_size_mb"] = round(stats["total_size_mb"], 2)
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# å…¨å±€æœåŠ¡å®ä¾‹
_log_export_service: Optional[LogExportService] = None


def get_log_export_service() -> LogExportService:
    """è·å–æ—¥å¿—å¯¼å‡ºæœåŠ¡å®ä¾‹"""
    global _log_export_service
    
    if _log_export_service is None:
        # ä»é…ç½®ä¸­è·å–æ—¥å¿—ç›®å½•
        from app.core.config import settings
        log_dir = settings.log_dir if hasattr(settings, 'log_dir') else "./logs"
        _log_export_service = LogExportService(log_dir=log_dir)
    
    return _log_export_service

